{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bayesian Workflow OR A Taste of Probabilistic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian parameter estimation using PyMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to generalize our new skills and build on them by learning the basics of probabilistic programming using PyMC. To this end, let's recap the general workflow. \n",
    "\n",
    "In the basics of Bayesian model building, the steps are\n",
    "1. To completely specify the model in terms of _probability distributions_. This includes specifying \n",
    "    - what the form of the sampling distribution of the data is _and_ \n",
    "    - what form describes our _uncertainty_ in the unknown parameters.\n",
    "2. Calculate the _posterior distribution_.\n",
    "\n",
    "In Chapter 5 and 6's biased coin flipping example, the form of the sampling distribution of the data was Binomial (described by the likelihood) and the uncertainty around the unknown parameter $p$ captured by the prior (we used both the uniform and the Jeffreys priors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to do the same using the **probabilistic programming language** PyMC. There's _loads of cool stuff_ about PyMC and this paradigm, two of which are\n",
    "- _probabililty distributions_ are first class citizens, in that we can assign them to variables and use them intuitively to mirror how we think about priors, likelihoods & posteriors.\n",
    "- PyMC calculates the posterior for us!\n",
    "\n",
    "Under the hood, PyMC will compute the posterior using a sampling based approach called Markov Chain Monte Carlo (MCMC) or Variational Inference. Check the [PyMC3 docs](https://docs.pymc.io/) for more on these. \n",
    "\n",
    "But now, it's time to bust out some MCMC and get sampling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter estimation I: click-through rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common experiment in tech data science is to test a product change and see how it affects a metric that you're interested in. Say that I don't think enough people are clicking a button on my website & I hypothesize that it's because the button is a similar color to the background of the page. Then I can set up two pages and send some people to each: the first the original page, the second a page that is identical, except that it has a button that is of higher contrast and see if more people click through. This is commonly referred to as an A/B test and the metric of interest is click-through rate (CTR), what proportion of people click through. Before even looking at two rates, let's use PyMC to estimate one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets generate click-through data, given a CTR $p_a=0.15$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| output: false\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from IPython.display import display_png\n",
    "from generative_thinking.utils import ecdf \n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:03:36.261316Z",
     "iopub.status.busy": "2022-04-30T07:03:36.261153Z",
     "iopub.status.idle": "2022-04-30T07:03:36.263972Z",
     "shell.execute_reply": "2022-04-30T07:03:36.263653Z"
    }
   },
   "outputs": [],
   "source": [
    "# click-through rates\n",
    "np.random.seed(42)\n",
    "p_a = 0.15\n",
    "N = 150\n",
    "n_successes_a = np.sum(np.random.binomial(N, p_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to build our probability model. Noticing that our model of having a constant CTR resulting in click or not is a biased coin flip,\n",
    "- the sampling distribution is binomial and we need to encode this in the likelihood;\n",
    "- there is a single parameter $p$ that we need to describe the uncertainty around, using a prior and we'll use a uniform prior for this.\n",
    "\n",
    "These are the ingredients for the model so let's now build it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:03:36.265796Z",
     "iopub.status.busy": "2022-04-30T07:03:36.265697Z",
     "iopub.status.idle": "2022-04-30T07:03:36.278819Z",
     "shell.execute_reply": "2022-04-30T07:03:36.278522Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build model of p_a\n",
    "with pm.Model() as model:\n",
    "    # Prior on p\n",
    "    prob = pm.Uniform('p')\n",
    "    # Binomial Likelihood\n",
    "    y = pm.Binomial('y', n=N, p=prob, observed=n_successes_a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've done the hard work of building our model, \n",
    "let's visualize the model graph using `graphviz`.\n",
    "In our case, the graph will look simple\n",
    "because our model doesn't have too many moving parts.\n",
    "That said, being able to see the model graph is awesome\n",
    "and will introduce you with a tool\n",
    "that will help visualizing more complex models in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:03:36.280658Z",
     "iopub.status.busy": "2022-04-30T07:03:36.280555Z",
     "iopub.status.idle": "2022-04-30T07:03:36.475266Z",
     "shell.execute_reply": "2022-04-30T07:03:36.474820Z"
    }
   },
   "outputs": [],
   "source": [
    "gv = pm.model_graph.model_to_graphviz(model)\n",
    "display_png(gv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to sample from the posterior using PyMC. You'll also then plot the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:03:36.477460Z",
     "iopub.status.busy": "2022-04-30T07:03:36.477344Z",
     "iopub.status.idle": "2022-04-30T07:04:07.910093Z",
     "shell.execute_reply": "2022-04-30T07:04:07.909733Z"
    }
   },
   "outputs": [],
   "source": [
    "#| output: false\n",
    "with model:\n",
    "    samples = pm.sample(2000, return_inferencedata=True)\n",
    "    # We're also sampling the prior/posterior predictives,\n",
    "    # which we'll introduce and explain below\n",
    "    idata = pm.sample_prior_predictive(model=model)\n",
    "    idata.extend(pm.sample(progressbar=False))\n",
    "    idata.extend(pm.sample_posterior_predictive(idata, progressbar=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:04:07.912450Z",
     "iopub.status.busy": "2022-04-30T07:04:07.912327Z",
     "iopub.status.idle": "2022-04-30T07:04:08.024448Z",
     "shell.execute_reply": "2022-04-30T07:04:08.024068Z"
    }
   },
   "outputs": [],
   "source": [
    "#| fig-cap: Posterior samples of $p$, the click-through rate, having seen the data.\n",
    "with model:\n",
    "    az.plot_posterior(samples, kind='hist');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictive checks and traceplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now introduce 3 diagnostic tools, which will come in handy when building models:\n",
    "\n",
    "- prior predictive check,\n",
    "- posterior predictive check, and\n",
    "- traceplots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prior predictive checks** essentially generate samples of the data\n",
    "based on your specified priors without looking at the actual data.\n",
    "They help you check whether your priors are sensible distributions,\n",
    "or if they have the potential to generate wildly unreasonable data.\n",
    "Let's plot the prior predictive check ECDF for our model above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:04:08.026666Z",
     "iopub.status.busy": "2022-04-30T07:04:08.026552Z",
     "iopub.status.idle": "2022-04-30T07:04:08.113013Z",
     "shell.execute_reply": "2022-04-30T07:04:08.112677Z"
    }
   },
   "outputs": [],
   "source": [
    "#| fig-cap: Prior predictive samples for observed data.\n",
    "# Generate x & y data for ECDF\n",
    "x_ecdf, y_ecdf = ecdf(idata.prior_predictive[\"y\"].data.flatten())\n",
    "# Plot the ECDF\n",
    "plt.plot(x_ecdf, y_ecdf, marker='.', linestyle='none');\n",
    "plt.xlabel(\"Number of Clicks\")\n",
    "plt.ylabel(\"Cumulative Fraction\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to ask the question: Does this prior predictive check look sensible?\n",
    "This question is both qualitative and quantitative.\n",
    "Qualitatively, it looks extremely similar to a uniform distribution's CDF.\n",
    "Quantitatively, its lower bound $0$ and upper bound $150$ are what we expect.\n",
    "We know that we are sampling the number of click throughs\n",
    "out of a total number of 150 website landings\n",
    "so this prior distribution looks pretty good!\n",
    "\n",
    "Counterfactually, we would be concerned if there negative values _or_ values over $150$,\n",
    "as they would violate what we assume about our data-generating processes.\n",
    "In the next example, we'll see how red flags in our prior predictive check\n",
    "can lead us to alter our priors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *posterior* predictive check samples our posterior distribution of data so we would expect it to look something like our actual data. Let's check it out, in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:04:08.114981Z",
     "iopub.status.busy": "2022-04-30T07:04:08.114862Z",
     "iopub.status.idle": "2022-04-30T07:04:08.188994Z",
     "shell.execute_reply": "2022-04-30T07:04:08.188599Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate x & y data for ECDF\n",
    "x_ecdf, y_ecdf = ecdf(idata.posterior_predictive[\"y\"].data.flatten())\n",
    "# Plot the ECDF\n",
    "plt.plot(x_ecdf, y_ecdf, marker='.', linestyle='none', color=\"black\")\n",
    "plt.axvline(x=n_successes_a, color=\"black\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this looks somewhat Gaussian, centered around 21. Now our data is a single number, the number of clicks, so we would expect it to be around $21\\pm5$. Let's see what it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:04:08.191459Z",
     "iopub.status.busy": "2022-04-30T07:04:08.191067Z",
     "iopub.status.idle": "2022-04-30T07:04:08.194182Z",
     "shell.execute_reply": "2022-04-30T07:04:08.193796Z"
    }
   },
   "outputs": [],
   "source": [
    "n_successes_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's now plot the *traceplot*, which plots our parameters as a function of iteration number and tells us about whether our sampling has converged. Essentially we want to make sure that the sampling procedure doesn't \"shoot off\" in any weird way, such as in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:04:08.195961Z",
     "iopub.status.busy": "2022-04-30T07:04:08.195858Z",
     "iopub.status.idle": "2022-04-30T07:04:08.211321Z",
     "shell.execute_reply": "2022-04-30T07:04:08.211016Z"
    }
   },
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "from IPython.display import Image\n",
    "Image(\"../../img/badchains.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we really want to see is a traceplot that doesn't exhibit drift but diffusion,\n",
    "that is, noise around a certain value.\n",
    "There are ways to frame this in terms of autocorrelation but,\n",
    "for the time being, the most important way to think about it is\n",
    "you want to see a traceplot that looks something like a \"hairy caterpillar\",\n",
    "which we do on the right in this case\n",
    "(on the left we have the samples of our distribution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:04:08.224767Z",
     "iopub.status.busy": "2022-04-30T07:04:08.224622Z",
     "iopub.status.idle": "2022-04-30T07:04:08.350494Z",
     "shell.execute_reply": "2022-04-30T07:04:08.350151Z"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_trace(samples);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter estimation II -- the mean of a population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now calculate the  posterior mean beak depth of Galapagos finches in a given species. First let's load the data and subset it with respect to species:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:04:08.352903Z",
     "iopub.status.busy": "2022-04-30T07:04:08.352803Z",
     "iopub.status.idle": "2022-04-30T07:04:08.359554Z",
     "shell.execute_reply": "2022-04-30T07:04:08.359239Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import and view head of data\n",
    "df_12 = pd.read_csv('../../datasets/finch_beaks_2012.csv')\n",
    "df_fortis = df_12.loc[df_12['species'] == 'fortis']\n",
    "df_scandens = df_12.loc[df_12['species'] == 'scandens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify the full probability model, we need\n",
    "- a likelihood function for the data &\n",
    "- priors for all unknowns.\n",
    "\n",
    "What is the likelihood here? Let's plot the measurements below and see that they look approximately normal so we'll use a normal likelihood $y_i\\sim \\mathcal{N}(\\mu, \\sigma^2)$. The unknowns here are the mean $\\mu$ and standard deviation $\\sigma$ and we'll use weakly informative priors on both\n",
    "\n",
    "- a normal prior for $\\mu$ with mean $10$ and standard deviation $5$;\n",
    "- a lognormal prior for $\\sigma$ with $\\mu = 0$ and $\\sigma = 10$.\n",
    "\n",
    "There are biological reasons for these priors \n",
    "(**NOTE: not really now, with the lognormal prior? \n",
    "Need to explain what the biological reasons are otherwise**)\n",
    "also but we can also test that the posteriors are relatively robust to the choice of prior here due to the amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:04:08.361612Z",
     "iopub.status.busy": "2022-04-30T07:04:08.361479Z",
     "iopub.status.idle": "2022-04-30T07:04:08.486076Z",
     "shell.execute_reply": "2022-04-30T07:04:08.485746Z"
    }
   },
   "outputs": [],
   "source": [
    "#| warnings:false\n",
    "sns.distplot(df_fortis['blength']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now build our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:04:08.488187Z",
     "iopub.status.busy": "2022-04-30T07:04:08.488089Z",
     "iopub.status.idle": "2022-04-30T07:04:08.505923Z",
     "shell.execute_reply": "2022-04-30T07:04:08.505630Z"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Prior for mean & standard deviation\n",
    "    mu_1 = pm.Normal('mu_1', mu=10, sigma=5)\n",
    "    sigma_1 = pm.Lognormal('sigma_1', 0, 10)\n",
    "    # Gaussian Likelihood\n",
    "    y_1 = pm.Normal('y_1', mu=mu_1, sigma=sigma_1,\n",
    "     observed=df_fortis['blength'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can also visualize our model graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:04:08.507902Z",
     "iopub.status.busy": "2022-04-30T07:04:08.507787Z",
     "iopub.status.idle": "2022-04-30T07:04:08.724910Z",
     "shell.execute_reply": "2022-04-30T07:04:08.724321Z"
    }
   },
   "outputs": [],
   "source": [
    "gv = pm.model_graph.model_to_graphviz(model)\n",
    "display_png(gv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now press our inference button and sample from the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:04:08.728025Z",
     "iopub.status.busy": "2022-04-30T07:04:08.727782Z",
     "iopub.status.idle": "2022-04-30T07:04:37.700011Z",
     "shell.execute_reply": "2022-04-30T07:04:37.699677Z"
    }
   },
   "outputs": [],
   "source": [
    "#| output: false\n",
    "with model:\n",
    "    samples = pm.sample(2000, return_inferencedata=True)\n",
    "    idata = pm.sample_prior_predictive(model=model)\n",
    "    idata.extend(pm.sample(progressbar=False))\n",
    "    idata.extend(pm.sample_posterior_predictive(idata, progressbar=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot our *prior predictive check*, data, and *posterior predictive check*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:04:37.702165Z",
     "iopub.status.busy": "2022-04-30T07:04:37.702055Z",
     "iopub.status.idle": "2022-04-30T07:04:37.796182Z",
     "shell.execute_reply": "2022-04-30T07:04:37.795570Z"
    }
   },
   "outputs": [],
   "source": [
    "x_ecdf, y_ecdf = ecdf(idata.prior_predictive[\"y_1\"].data.flatten())\n",
    "# Plot the ECDF\n",
    "plt.plot(x_ecdf, y_ecdf, marker='.', linestyle='none');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:04:37.798410Z",
     "iopub.status.busy": "2022-04-30T07:04:37.798279Z",
     "iopub.status.idle": "2022-04-30T07:04:37.884162Z",
     "shell.execute_reply": "2022-04-30T07:04:37.883706Z"
    }
   },
   "outputs": [],
   "source": [
    "x_ecdf, y_ecdf = ecdf(df_fortis['blength'].values)\n",
    "# Plot the ECDF\n",
    "plt.plot(x_ecdf, y_ecdf, marker='.', linestyle='none');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:04:37.886316Z",
     "iopub.status.busy": "2022-04-30T07:04:37.886146Z",
     "iopub.status.idle": "2022-04-30T07:04:38.147748Z",
     "shell.execute_reply": "2022-04-30T07:04:38.147434Z"
    }
   },
   "outputs": [],
   "source": [
    "x_ecdf, y_ecdf = ecdf(idata.posterior_predictive[\"y_1\"].data.flatten())\n",
    "# Plot the ECDF\n",
    "plt.plot(x_ecdf, y_ecdf, marker='.', linestyle='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the *posterior predictive check* passes the eyeball test (when comapring with the data), the *prior predictive check* does not! Looking at the x-axis on it, we see that we end up with values orders of magnitude larger than what a beak length could possibly be!\n",
    "\n",
    "This means that our priors aren't quite right yet.\n",
    "Looking at them, we can see that having a Lognormal prior for our variance\n",
    "means that we could end up with some pretty large values\n",
    "so let's try a prior that drops off more quickly (we say it has a _shorter tail_).\n",
    "We'll try an Exponential prior here as we know that these have shorter tails.\n",
    "We don't expect the reader to be aware of this\n",
    "but we hope to help build your intuition around this here\n",
    "and in the following case stories!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:04:38.149719Z",
     "iopub.status.busy": "2022-04-30T07:04:38.149597Z",
     "iopub.status.idle": "2022-04-30T07:04:38.162292Z",
     "shell.execute_reply": "2022-04-30T07:04:38.162002Z"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Prior for mean & standard deviation\n",
    "    mu_1 = pm.Normal('mu_1', mu=10, sigma=5)\n",
    "    sigma_1 = pm.Exponential('sigma_1', lam=0.5)\n",
    "    # Gaussian Likelihood\n",
    "    y_1 = pm.Normal('y_1', mu=mu_1, sigma=sigma_1, \n",
    "    observed=df_fortis['blength'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:04:38.164071Z",
     "iopub.status.busy": "2022-04-30T07:04:38.163989Z",
     "iopub.status.idle": "2022-04-30T07:05:06.470933Z",
     "shell.execute_reply": "2022-04-30T07:05:06.470292Z"
    }
   },
   "outputs": [],
   "source": [
    "#| output: false\n",
    "with model:\n",
    "    samples = pm.sample(2000, return_inferencedata=True)\n",
    "    idata = pm.sample_prior_predictive(model=model)\n",
    "    idata.extend(pm.sample(progressbar=False))\n",
    "    idata.extend(pm.sample_posterior_predictive(idata, \n",
    "        progressbar=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot our *prior* and *posterior* predictive checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:05:06.473133Z",
     "iopub.status.busy": "2022-04-30T07:05:06.473001Z",
     "iopub.status.idle": "2022-04-30T07:05:06.575678Z",
     "shell.execute_reply": "2022-04-30T07:05:06.575268Z"
    }
   },
   "outputs": [],
   "source": [
    "x_ecdf, y_ecdf = ecdf(idata.prior_predictive[\"y_1\"].data.flatten())\n",
    "# Plot the ECDF\n",
    "plt.plot(x_ecdf, y_ecdf, marker='.', linestyle='none');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:05:06.577750Z",
     "iopub.status.busy": "2022-04-30T07:05:06.577650Z",
     "iopub.status.idle": "2022-04-30T07:05:06.830394Z",
     "shell.execute_reply": "2022-04-30T07:05:06.829984Z"
    }
   },
   "outputs": [],
   "source": [
    "x_ecdf, y_ecdf = ecdf(idata.posterior_predictive[\"y_1\"].data.flatten())\n",
    "# Plot the ECDF\n",
    "plt.plot(x_ecdf, y_ecdf, marker='.', linestyle='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *prior* predictive check is now looking a lot better!\n",
    "There is still the possibility of negative values, so that may still be an issue.\n",
    "Additionally, if it were still generating data that are too large,\n",
    "we could increase the exponent `lam`, among other things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the posteriors of our two parameters, mean $\\mu$ and standard deviation $\\sigma$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:05:06.832600Z",
     "iopub.status.busy": "2022-04-30T07:05:06.832464Z",
     "iopub.status.idle": "2022-04-30T07:05:06.942120Z",
     "shell.execute_reply": "2022-04-30T07:05:06.941667Z"
    }
   },
   "outputs": [],
   "source": [
    "with model:\n",
    "    az.plot_posterior(samples);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We once again plot the traceplot, which plots our parameters as a function of iteration number, showing that our model has converged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:05:06.944271Z",
     "iopub.status.busy": "2022-04-30T07:05:06.944167Z",
     "iopub.status.idle": "2022-04-30T07:05:07.211815Z",
     "shell.execute_reply": "2022-04-30T07:05:07.211264Z"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_trace(samples, figsize=(10, 7));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After parameter estimation, a common scientific question involves hypothesis testing, so let's now see how Bayesian inference and probabilistic programming can be used for hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian hypothesis testing I: A/B tests on click through rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we have a website and want to redesign the layout (*A*) and test whether the new layout (*B*) results in a higher click through rate. When people come to our website we randomly show them layout *A* or *B* and see how many people click through for each. First let's generate the data we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:05:07.215919Z",
     "iopub.status.busy": "2022-04-30T07:05:07.215769Z",
     "iopub.status.idle": "2022-04-30T07:05:07.219078Z",
     "shell.execute_reply": "2022-04-30T07:05:07.218599Z"
    }
   },
   "outputs": [],
   "source": [
    "# click-through rates\n",
    "p_a = 0.15\n",
    "p_b = 0.20\n",
    "N = 1000\n",
    "n_successes_a = np.sum(np.random.uniform(size=N) <= p_a)\n",
    "n_successes_b = np.sum(np.random.uniform(size=N) <= p_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we need to specify our models for $p_a$ and $p_b$. Each will be the same as the CTR example above\n",
    "- Binomial likelihoods\n",
    "- uniform priors on $p_a$ and $_p$.\n",
    "\n",
    "We also want to calculate the posterior of the difference $p_a-p_b$ and we do so using `pm.Deterministic()`, which specifies a deterministic random variable, i.e., one that is completely determined by the values it references, in the case $p_a$ & $p_b$.\n",
    "\n",
    "We'll now build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:05:07.221532Z",
     "iopub.status.busy": "2022-04-30T07:05:07.221388Z",
     "iopub.status.idle": "2022-04-30T07:05:07.232365Z",
     "shell.execute_reply": "2022-04-30T07:05:07.231899Z"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Prior on p\n",
    "    prob_a = pm.Uniform('p_a')\n",
    "    prob_b = pm.Uniform('p_b')\n",
    "    # Binomial Likelihood\n",
    "    y_a = pm.Binomial('y_a', n=N, p=prob_a, observed=n_successes_a)\n",
    "    y_b = pm.Binomial('y_b', n=N, p=prob_b, observed=n_successes_b)\n",
    "    diff_clicks = pm.Deterministic('diff_clicks', prob_a-prob_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot our model graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:05:07.234650Z",
     "iopub.status.busy": "2022-04-30T07:05:07.234508Z",
     "iopub.status.idle": "2022-04-30T07:05:07.445397Z",
     "shell.execute_reply": "2022-04-30T07:05:07.444823Z"
    }
   },
   "outputs": [],
   "source": [
    "gv = pm.model_graph.model_to_graphviz(model)\n",
    "display_png(gv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample from the posterior and plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:05:07.448531Z",
     "iopub.status.busy": "2022-04-30T07:05:07.448328Z",
     "iopub.status.idle": "2022-04-30T07:05:23.321707Z",
     "shell.execute_reply": "2022-04-30T07:05:23.321369Z"
    }
   },
   "outputs": [],
   "source": [
    "#| output: false\n",
    "with model:\n",
    "    samples = pm.sample(2000, return_inferencedata=True)\n",
    "    az.plot_posterior(samples, kind='hist');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We once again plot the traceplot, which plots our parameters as a function of iteration number, showing that our model has converged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:05:23.324203Z",
     "iopub.status.busy": "2022-04-30T07:05:23.324032Z",
     "iopub.status.idle": "2022-04-30T07:05:23.692700Z",
     "shell.execute_reply": "2022-04-30T07:05:23.692325Z"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_trace(samples, figsize=(10, 7));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, for the sake of brevity, we did not perform our predictive checks, but this is something that the excited reader can do, in this case, and for those below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian hypothesis testing II -- beak lengths difference between species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we seek to determine whether the mean beak length of the Galapagos finches differs between species. For the mean of each species, we use the same model as in previous section:\n",
    "\n",
    "- Gaussian likelihood;\n",
    "- Normal prior for the means;\n",
    "- Uniform prior for the variances.\n",
    "\n",
    "We also calculate the difference between the means and, the _effect size_, which is the difference between the means divided by the pooled standard deviations = $\\sqrt{(\\sigma_1^2+\\sigma_2^2)/2}$. \n",
    "\n",
    "We then sample from the posteriors and plot them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:05:23.695827Z",
     "iopub.status.busy": "2022-04-30T07:05:23.695685Z",
     "iopub.status.idle": "2022-04-30T07:05:23.724423Z",
     "shell.execute_reply": "2022-04-30T07:05:23.723798Z"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Priors for means and variances\n",
    "    mu_1 = pm.Normal('mu_1', mu=10, sigma=5)\n",
    "    sigma_1 = pm.Uniform('sigma_1', 0, 10)\n",
    "    mu_2 = pm.Normal('mu_2', mu=10, sigma=5)\n",
    "    sigma_2 = pm.Uniform('sigma_2', 0, 10)\n",
    "    # Gaussian Likelihoods\n",
    "    y_1 = pm.Normal('y_1', mu=mu_1, sigma=sigma_1, observed=df_fortis['blength'])\n",
    "    y_2 = pm.Normal('y_2', mu=mu_2, sigma=sigma_2, observed=df_scandens['blength'])\n",
    "    # Calculate the effect size and its uncertainty.\n",
    "    diff_means = pm.Deterministic('diff_means', mu_1 - mu_2)\n",
    "    pooled_sd = pm.Deterministic('pooled_sd', \n",
    "                                 np.sqrt(np.power(sigma_1, 2) + \n",
    "                                         np.power(sigma_2, 2)) / 2)\n",
    "    effect_size = pm.Deterministic('effect_size', \n",
    "                                   diff_means / pooled_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot our model graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:05:23.727140Z",
     "iopub.status.busy": "2022-04-30T07:05:23.726984Z",
     "iopub.status.idle": "2022-04-30T07:05:23.949966Z",
     "shell.execute_reply": "2022-04-30T07:05:23.949552Z"
    }
   },
   "outputs": [],
   "source": [
    "gv = pm.model_graph.model_to_graphviz(model)\n",
    "display_png(gv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:05:23.952269Z",
     "iopub.status.busy": "2022-04-30T07:05:23.952106Z",
     "iopub.status.idle": "2022-04-30T07:05:40.275112Z",
     "shell.execute_reply": "2022-04-30T07:05:40.274744Z"
    }
   },
   "outputs": [],
   "source": [
    "#| output: false\n",
    "with model:\n",
    "    samples = pm.sample(2000, return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:05:40.277255Z",
     "iopub.status.busy": "2022-04-30T07:05:40.277126Z",
     "iopub.status.idle": "2022-04-30T07:05:40.507192Z",
     "shell.execute_reply": "2022-04-30T07:05:40.506776Z"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_posterior(samples, \n",
    "    var_names=['mu_1', 'mu_2', 'diff_means', 'effect_size'], kind='hist');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:05:40.509062Z",
     "iopub.status.busy": "2022-04-30T07:05:40.508972Z",
     "iopub.status.idle": "2022-04-30T07:05:41.253222Z",
     "shell.execute_reply": "2022-04-30T07:05:41.252900Z"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_trace(samples, figsize=(10, 7));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian regression\n",
    "\n",
    "In the above, we have seen how to use the Bayesian workflow and probabilistic programming for both parameter estimation and comparison between groups. To now round out our first few Bayesian modeling tools, let's see how to use the Bayesian workflow and probabilistic programming for linear regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end, let's first generate some bivariate data, consisting of a linear relationship and a noise/error term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:05:41.256159Z",
     "iopub.status.busy": "2022-04-30T07:05:41.256050Z",
     "iopub.status.idle": "2022-04-30T07:05:41.437471Z",
     "shell.execute_reply": "2022-04-30T07:05:41.437102Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "(m, b) = (25, 0.5)\n",
    "x = 100 * np.random.random(20)\n",
    "y = m * x + b\n",
    "\n",
    "# add scatter to points\n",
    "x = np.random.normal(x, 10)\n",
    "y = np.random.normal(y, 10)\n",
    "\n",
    "plt.plot(x, y, 'ok');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now recall that our Bayesian model building steps are\n",
    "1. To completely specify the model in terms of _probability distributions_. This includes specifying \n",
    "    - what the form of the sampling distribution of the data is _and_ \n",
    "    - what form describes our _uncertainty_ in the unknown parameters.\n",
    "2. Calculate the _posterior distribution_.\n",
    "\n",
    "The sampling distribution of the data is given by $y = mx + b + \\sigma$, where $m$ is the gradient, $b$ the $y$-intercept, and $\\sigma$ the error/noise term. These are the unknown parameters, which we need to describe our _uncertatinty_ around using priors and we choose:\n",
    "\n",
    "- Gaussian priors for $m$ and $b$,\n",
    "- A Half-Cauchy prior for $\\sigma$ (this is a distribution that has desirable properties for us, but isn't worth getting into the details of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:05:41.439730Z",
     "iopub.status.busy": "2022-04-30T07:05:41.439535Z",
     "iopub.status.idle": "2022-04-30T07:05:41.458095Z",
     "shell.execute_reply": "2022-04-30T07:05:41.457740Z"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Define priors\n",
    "    sigma = pm.HalfCauchy(\"sigma\", beta=10)\n",
    "    b = pm.Normal(\"b\", 0, sigma=20)\n",
    "    m = pm.Normal(\"m\", 0, sigma=20)\n",
    "\n",
    "    # Define likelihood\n",
    "    likelihood = pm.Normal(\"y\", mu=b + m * x, sigma=sigma, observed=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot our model graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:05:41.460299Z",
     "iopub.status.busy": "2022-04-30T07:05:41.460008Z",
     "iopub.status.idle": "2022-04-30T07:05:41.653671Z",
     "shell.execute_reply": "2022-04-30T07:05:41.653164Z"
    }
   },
   "outputs": [],
   "source": [
    "gv = pm.model_graph.model_to_graphviz(model)\n",
    "display_png(gv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:05:41.655962Z",
     "iopub.status.busy": "2022-04-30T07:05:41.655850Z",
     "iopub.status.idle": "2022-04-30T07:05:56.736296Z",
     "shell.execute_reply": "2022-04-30T07:05:56.735967Z"
    }
   },
   "outputs": [],
   "source": [
    "#| output: false\n",
    "with model:\n",
    "    samples = pm.sample(2000, return_inferencedata=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll see in following chapter how to plot and interpret predictive checks for regression tasks, such as this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot the traceplot, which plots our parameters as a function of iteration number and here shows that our model has converged in a desirable way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T07:05:56.738504Z",
     "iopub.status.busy": "2022-04-30T07:05:56.738393Z",
     "iopub.status.idle": "2022-04-30T07:05:57.106856Z",
     "shell.execute_reply": "2022-04-30T07:05:57.106514Z"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_trace(samples, figsize=(10, 7));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we've introduced the fundamentals of the principled Bayesian workflow and demonstrated its utility in answering the common scientific questions of parameter estimation and hypothesis testing. We've done so for two different types of data, the first an example of data from A/B testing, the second a dataset from basic research of Galapagos finch beak measurements. We also introduced Bayesian linear regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
