{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import theano.tensor as tt\n",
    "from data import load_baseball\n",
    "from utils import despine, ECDF, despine_traceplot\n",
    "import arviz as az\n",
    "\n",
    "# For deterministic reproducibility.\n",
    "np.random.seed(42)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Baseball players have many metrics measured for them. Let's say we are on a baseball team, and would like to quantify player performance, one metric being their batting average (defined by how many times a batter hit a pitched ball, divided by the number of times they were up for batting (\"at bat\")). How would you go about this task?\n",
    "\n",
    "**Discuss**: \n",
    "1. What data would we need?\n",
    "1. What metric would you rank by? \n",
    "1. Would your metric be reasonable for rookie players?\n",
    "\n",
    "Let's read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_baseball()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Betas, Bernoullis and Binomials: A brief introduction\n",
    "\n",
    "- Bernoulli distribution: a probability distribution modelling one \"coin flip\"-like trial. Parameterized by a single parameter `p`, which indicates probability of \"success\".\n",
    "- Binomial distribution: a probability distribution modelling the number of successes in `n` trials. Parameterized by both `n` and `p`.\n",
    "- Beta distributions: a probability distribution bounded over the interval $(0, 1)$. Models distribution of probability values, usually the `p` in a Bernoulli or Binomial. Parameterized by $\\alpha$ and $\\beta$, which can be thought of as \"number of successes\" and \"number of failures\" respectively.\n",
    "\n",
    "Every distribution has its \"story\". If you're curious, check out [Justin Bois' probability stories][probstory] page.\n",
    "\n",
    "[probstory]: http://bebi103.caltech.edu.s3-website-us-east-1.amazonaws.com/2017/tutorials/t3b_probability_stories.html#Beta-distribution\n",
    "\n",
    "### Focus on beta\n",
    "\n",
    "Let's say we wanted to model a probability distribution centered approximately on 0.2. Depending on our parameterization of the Beta distribution, we can express different levels of confidence (as measured by the spread of the distribution) as to how sure we are a distribution takes on that value.\n",
    "\n",
    "The code + chart below should illustrate this clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To correctly express the beta distribution, we subtract 1.\n",
    "low_prior = np.random.beta(a=1, b=7, size=100000)\n",
    "med_prior = np.random.beta(a=19, b=79, size=100000)\n",
    "high_prior = np.random.beta(a=199, b=799, size=100000)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "labels = ['low', 'med', 'high']\n",
    "priors = [low_prior, med_prior, high_prior]\n",
    "\n",
    "for label, prior in zip(labels, priors):\n",
    "    x, y = ECDF(prior)\n",
    "    ax.plot(x, y, label=label)\n",
    "ax.legend()\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_xlabel('probability parameter')\n",
    "ax.set_ylabel('cumulative fraction')\n",
    "despine(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how increasing the scale of parameterization (1x, 10x, 100x) decreased the variance. This leads to another way of looking at the Beta distribution:\n",
    "\n",
    "- $\\alpha$ is the probability of success $p$ multiplied by a scale factor $\\kappa$.\n",
    "- $\\beta$ is $1-p$ multiplied by the same scale factor $\\kappa$.\n",
    "\n",
    "We will see this in action below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Write a naive estimation model for the players above.\n",
    "\n",
    "Hint, a possible model you could specify is as follows:\n",
    "\n",
    "![](../images/baseball-model.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as baseline_model:\n",
    "    # Define a prior distribution for the probability parameter `p`, called\n",
    "    # `thetas`, which is drawn from a Beta(0, 0) distribution, and is of\n",
    "    # the same length as the \n",
    "    thetas = \n",
    "    # Define a likelihood function, which is the Binomial distribution,\n",
    "    # in which `n` is the number of observed \"at bats\", p is `theta`, and \n",
    "    # the data we observe is the number of hits for that player.\n",
    "    like = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, sample from the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the traceplots to check for convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the posterior distribution using forest plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylabels = ylabels = \"AB: \" + df['AB'].astype(str) + ', H: ' + df['H'].astype('str')\n",
    "# Your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss:** Are the estimates reasonable, particularly for players that have had only one at bat (AB)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Modelling\n",
    "\n",
    "**Discuss:** \n",
    "- How do we deal with the fact that some players have only had 1 at bat (AB = 1), and zero hits (H = 0)? \n",
    "- Would it be reasonable, fair, and in line with prior knowledge that the player's true batting average was zero? \n",
    "\n",
    "## Assumptions\n",
    "\n",
    "If it is qualitatively (and maybe also quantitatively) justifiable, we can impose the following assumption on the modelling process.\n",
    "\n",
    "We can assume that there is some underlying distribution for a player's batting average. We can then assume that each new player draws his own batting average from the underlying distribution. This encodes our intuition that human performance falls on some continuum that has a region of \"concentrated\" average. (Think \"normal-like\" distribution.)\n",
    "\n",
    "The modelling choices for the hierarchical model below are detailed in the [PyMC3 docs](https://docs.pymc.io/notebooks/hierarchical_partial_pooling.html), but are copied below for convenience. \n",
    "\n",
    "> We will assume that there exists a hidden factor (phi) related to the expected performance for all players (not limited to our 18). Since the population mean is an unknown value between 0 and 1, it must be bounded from below and above. Also, we assume that nothing is known about global average. Hence, a natural choice for a prior distribution is the uniform distribution.\n",
    "> \n",
    "> Next, we introduce a hyperparameter kappa to account for the variance in the population batting averages, for which we will use a bounded Pareto distribution. This will ensure that the estimated value falls within reasonable bounds. These hyperparameters will be, in turn, used to parameterize a beta distribution, which is ideal for modeling quantities on the unit interval. The beta distribution is typically parameterized via a scale and shape parameter, it may also be parametrized in terms of its mean $\\mu \\in [0, 1]$ and sample size (a proxy for variance) $\\nu = \\alpha + \\beta (\\nu > 0)$\n",
    ">\n",
    "> The final step is to specify a sampling distribution for the data (hit or miss) for every player, using a Binomial distribution. This is where the data are brought to bear on the model.\n",
    "> \n",
    "> We could use `pm.Pareto('kappa', m=1.5)`, to define our prior on kappa, but the Pareto distribution has very long tails. Exploring these properly is difficult for the sampler, so we use an equivalent but faster parametrization using the exponential distribution. We use the fact that the log of a Pareto distributed random variable follows an exponential distribution.\n",
    "\n",
    "The most important thing to note is how the ${\\theta}$ parameters are drawn from a parental distribution that is not broadcast across the samples.\n",
    "\n",
    "**Notes:** \n",
    "\n",
    "- If you recall the parameterization of the beta distribution described above, it is exactly $p$ and $1-p$ multiplied by a scalar $\\kappa$! In the model spec below, we are treating phi and kappa as being parameters to estimate.\n",
    "- The model below expresses the intuition that the scale parameter is Pareto distributed - the intuition being that approx. 80% of the distribution is going to be small, but 20% of the time it'll be large.\n",
    "\n",
    "In visual form, this is the model below.\n",
    "\n",
    "![](../images/baseball-hierarchical-model.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as baseball_model:\n",
    "    \n",
    "    phi = pm.Uniform('phi', lower=0.0, upper=1.0)\n",
    "    kappa_log = pm.Exponential('kappa_log', lam=1.5)\n",
    "    kappa = pm.Deterministic('kappa', tt.exp(kappa_log))\n",
    "    \n",
    "    thetas = pm.Beta('thetas', alpha=phi*kappa, beta=(1.0-phi)*kappa, shape=len(df))\n",
    "    like = pm.Binomial('like', n=df['AB'], p=thetas, observed=df['H'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample from the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the trace plots to check for convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the posterior distributions using forestplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylabels = \"AB: \" + df['AB'].astype(str) + ', H: ' + df['H'].astype('str')\n",
    "# Your code below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a hierarchical model, we make the assumption that our observations (or treatments that group our observations) are somehow related. Under this assumption, when we have a new sample for which we have very few observations, we are able to borrow power from the population to make inferences about the new sample. \n",
    "\n",
    "Depending on the scenario, this assumption can either be reasonable, thereby not necessitating much debate, or be considered a \"strong assumption\", thereby requiring strong justification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrinkage\n",
    "\n",
    "\"Shrinkage\" is a term used to describe how hierarchical model estimation will usually result in parameter estimates that are \"shrunk\" away from their maximum likelihood estimators (i.e. the naive estimate from the data) towards the global mean. \n",
    "\n",
    "Shrinkage in and of itself is not necessarily a good or bad thing. However, because hierarchical models can sometimes be tricky to get right, we can use a shrinkage plot as a visual diagnostic for whether we have implemented the model correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE per player\n",
    "no_pool = df['batting_avg'].values\n",
    "# Hierarchical model\n",
    "partial_pool = trace['thetas'].mean(axis=0)\n",
    "# MLE over all players\n",
    "complete_pool = np.array([df['batting_avg'].mean()] * len(df))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for r in np.vstack([no_pool, partial_pool, complete_pool]).T:\n",
    "    ax.plot(r)\n",
    "    \n",
    "ax.set_xticks([0, 1, 2])\n",
    "ax.set_xticklabels(['no pooling', 'partial pooling', 'complete pooling'])\n",
    "ax.set_ylim(0, 1)\n",
    "despine(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- Hierarchical modelling assumes a \"hierarchical structure\" that governs relatedness between our observed samples.\n",
    "- Under the assumption of hierarchical structure holding true, we will not get estimates that one may consider to be absurd (e.g. long-run batting probability estimated to be zero or one, on the basis onf few observations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Exercises\n",
    "\n",
    "- Change the priors. How do they change the posteriors?\n",
    "- Change the parameterization of the model. How do they change your inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits\n",
    "\n",
    "## Model Credits\n",
    "\n",
    "This model implementation is taken from the [PyMC3 docs](https://docs.pymc.io/notebooks/hierarchical_partial_pooling.html).\n",
    "\n",
    "## Data Credits\n",
    "\n",
    "The data come from the [Baseball Data Bank](https://github.com/chadwickbureau/baseballdatabank), and are redistributed with this repository for educational purposes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian-modelling-tutorial",
   "language": "python",
   "name": "bayesian-modelling-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
