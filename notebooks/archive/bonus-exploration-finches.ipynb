{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from data import load_finches_2012, load_finches_1975\n",
    "from utils import ECDF\n",
    "import arviz as az\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I would like to write an estimation model for beak shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12 = load_finches_2012()\n",
    "df12['shape'] = df12['beak_depth'] / df12['beak_length']\n",
    "\n",
    "df12 = df12[df12['species'] != 'unknown']\n",
    "df75 = load_finches_1975()\n",
    "\n",
    "df = df12  # convenient alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fortis_idx = df[df['species'] == 'fortis'].index\n",
    "scandens_idx = df[df['species'] == 'scandens'].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model : Naive Division of Posteriors\n",
    "\n",
    "- Estimate posterior for depth and length independently.\n",
    "- Use posterior samples to estimate distribution for shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mega-model incorporating shape as well. \n",
    "# We will also analyze the SD in addition to the mean.\n",
    "\n",
    "with pm.Model() as beak_model:\n",
    "    # SD can only be positive, therefore it is reasonable to constrain to >0\n",
    "    # Likewise for betas.\n",
    "    sd_hyper = pm.HalfCauchy('sd_hyper', beta=100, shape=(2,))\n",
    "    beta_hyper = pm.HalfCauchy('beta_hyper', beta=100, shape=(2,))\n",
    "    \n",
    "    # Beaks cannot be of \"negative\" mean, therefore, HalfNormal is \n",
    "    # a reasonable, constrained prior.\n",
    "    mean_depth = pm.HalfNormal('mean_depth', sd=sd_hyper[0], shape=(2,))\n",
    "    sd_depth = pm.HalfCauchy('sd_depth', beta=beta_hyper[0], shape=(2,))\n",
    "    \n",
    "    mean_length = pm.HalfNormal('mean_length', sd=sd_hyper[1], shape=(2,))\n",
    "    sd_length = pm.HalfCauchy('sd_length', beta=beta_hyper[1], shape=(2,))\n",
    "\n",
    "    nu = pm.Exponential('nu', lam=1/29.) + 1\n",
    "    \n",
    "    # Define the likelihood distribution for the data.\n",
    "    depth = pm.StudentT('beak_depth', \n",
    "                        nu=nu,\n",
    "                        mu=mean_depth[df['species_enc']], \n",
    "                        sd=sd_depth[df['species_enc']], \n",
    "                        observed=df['beak_depth'])\n",
    "    \n",
    "    length = pm.StudentT('beak_length',\n",
    "                         nu=nu,\n",
    "                         mu=mean_length[df['species_enc']],\n",
    "                         sd=sd_length[df['species_enc']],\n",
    "                         observed=df['beak_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with beak_model:\n",
    "    trace = pm.sample(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, var_names=['mean_length', 'mean_depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace, varnames=['sd_length', 'sd_depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pm.sample_ppc(trace, model=beak_model)\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPC check for Fortis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122, sharex=ax1)\n",
    "\n",
    "def plot_ppc_data(samples, df, idxs, column, ax):\n",
    "    x, y = ECDF(samples[column][:, idxs].flatten())\n",
    "    ax.plot(x, y, label='ppc')\n",
    "    x, y = ECDF(df.iloc[idxs][column])\n",
    "    ax.plot(x, y, label='data')\n",
    "    ax.set_xlabel(column)\n",
    "    ax.set_ylabel('cumulative fraction')\n",
    "    return ax\n",
    "\n",
    "ax1 = plot_ppc_data(samples, df, fortis_idx, 'beak_depth', ax1)\n",
    "ax2 = plot_ppc_data(samples, df, fortis_idx, 'beak_length', ax2)\n",
    "\n",
    "fig.suptitle('Fortis')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPC check for Scandens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122, sharex=ax1)\n",
    "\n",
    "ax1 = plot_ppc_data(samples, df, scandens_idx, 'beak_depth', ax1)\n",
    "ax2 = plot_ppc_data(samples, df, scandens_idx, 'beak_length', ax2)\n",
    "\n",
    "fig.suptitle('Scandens')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x, y = ECDF((samples['beak_depth'][:, fortis_idx] / samples['beak_length'][:, fortis_idx]).flatten())\n",
    "ax.plot(x, y)\n",
    "x, y = ECDF(df.loc[fortis_idx, 'shape'])\n",
    "ax.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, looks like this is not the right model. Dividing PPC samples is definitely not the right approach here.\n",
    "\n",
    "Maybe jointly modelling the observed beak and length distributions is the right thing to do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "def plot_length_depth_scatter(df, idxs, title, ax):\n",
    "    ax.scatter(df.iloc[idxs]['beak_length'], df.iloc[idxs]['beak_depth'])\n",
    "    ax.set_xlabel('beak_length')\n",
    "    ax.set_ylabel('beak_depth')\n",
    "    ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1 = plot_length_depth_scatter(df, scandens_idx, 'scandens', ax1)\n",
    "\n",
    "ax2 = fig.add_subplot(122, sharex=ax1, sharey=ax1)\n",
    "ax2 = plot_length_depth_scatter(df, fortis_idx, 'fortis', ax2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Joint Distribution\n",
    "\n",
    "Going to try a new model: we explicity model depth and length jointly, as a multivariate gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as mv_beaks:  # multivariate beak model\n",
    "    packed_L = pm.LKJCholeskyCov('packed_L', n=2,\n",
    "                             eta=2., sd_dist=pm.HalfCauchy.dist(2.5))\n",
    "    L = pm.expand_packed_triangular(2, packed_L)\n",
    "    sigma = pm.Deterministic('sigma', L.dot(L.T))\n",
    "\n",
    "    mu = pm.HalfNormal('mu', sd=20, shape=(2,))\n",
    "    \n",
    "    like = pm.MvNormal('like', mu=mu, cov=sigma, observed=df.iloc[scandens_idx][['beak_depth', 'beak_length']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mv_beaks:\n",
    "    trace_mv = pm.sample(2000, njobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_mv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(trace_mv, var_names=['sigma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(trace_mv, var_names=['mu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_mv = pm.sample_ppc(trace_mv, model=mv_beaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_mv['like'][:, 0]  # beak_depth\n",
    "samples_mv['like'][:, 1]  # beak_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "x, y = ECDF(samples_mv['like'][:, 0])\n",
    "ax1.plot(x, y, label='ppc')\n",
    "x, y = ECDF(df.iloc[scandens_idx]['beak_depth'])\n",
    "ax1.plot(x, y, label='data')\n",
    "ax1.set_title('beak depth')\n",
    "ax1.legend()\n",
    "\n",
    "x, y = ECDF(samples_mv['like'][:, 1])\n",
    "ax2.plot(x, y, label='ppc')\n",
    "x, y = ECDF(df.iloc[scandens_idx]['beak_length'])\n",
    "ax2.plot(x, y, label='data')\n",
    "ax2.set_title('beak length')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "x, y = ECDF(trace_mv['sigma'][:, 0, 1])\n",
    "ax.plot(x, y, label='samples')\n",
    "x, y = ECDF(df.iloc[scandens_idx]['shape'])\n",
    "ax.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Regress Depth on Length\n",
    "\n",
    "Maybe the right way to compute shape is to regress depth on length, and compute the slope. After all, that's all that depth/length really is.\n",
    "\n",
    "We will assume a model: $y=mx$, no intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as shape_model:\n",
    "    shape = pm.Normal('shape', mu=0, sd=100)\n",
    "    sd = pm.HalfCauchy('sd', beta=100)\n",
    "    \n",
    "    mu = shape * df.iloc[scandens_idx]['beak_length'].values\n",
    "    \n",
    "    like = pm.Normal('like', mu=mu, sd=sd, observed=df.iloc[scandens_idx]['beak_depth'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with shape_model:\n",
    "    trace_shape = pm.sample(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "x, y = ECDF(trace_shape['shape'])\n",
    "ax.plot(x, y, label='sample')\n",
    "x, y = ECDF(df.iloc[scandens_idx]['shape'].values)\n",
    "ax.plot(x, y, label='data')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have the model mis-specified - I get the posterior distribution over the slope, but not the distribution of shapes. I guess shapes and slopes are kind of different. \n",
    "\n",
    "Let's try just estimating shape directly.\n",
    "\n",
    "# Model: Estimate on Shape Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as shape_model:\n",
    "    mu = pm.HalfNormal('mu', sd=100)\n",
    "    sd = pm.HalfCauchy('sd', beta=100)\n",
    "    \n",
    "    like = pm.Normal('shape', mu=mu, sd=sd, observed=df.iloc[scandens_idx]['shape'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with shape_model:\n",
    "    trace = pm.sample(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pm.sample_ppc(trace, model=shape_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "x, y = ECDF(samples['shape'].flatten())\n",
    "ax.plot(x, y, label='samples')\n",
    "x, y = ECDF(df.iloc[scandens_idx]['shape'])\n",
    "ax.plot(x, y, label='data')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, the simplest model is the best fitting one..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian-modelling-tutorial",
   "language": "python",
   "name": "bayesian-modelling-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
