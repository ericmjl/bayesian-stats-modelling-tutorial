{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import janitor as jn\n",
    "import pymc3 as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from utils import ECDF\n",
    "import arviz as az\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Goals\n",
    "\n",
    "In this notebookï¼Œyou will: \n",
    "\n",
    "1. Continue to gain familiarity with describing the data generation process using PyMC3 probability distributions.\n",
    "1. Explore a different probability distribution story using the Student T family of distributions.\n",
    "1. Gain practice writing hierarchical models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Darwin's Finches\n",
    "\n",
    "A research group has taken measurements of the descendants of the finches that Charles Darwin observed when he postulated the theory of evolution.\n",
    "\n",
    "We will be using Bayesian methods to analyze this data, specifically answering the question of how quantitatively different two species of birds' beaks are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Credits:** The Darwin's finches datasets come from the paper, [40 years of evolution. Darwin's finches on Daphne Major Island][data]. \n",
    "\n",
    "One row of data has been added for pedagogical purposes.\n",
    "\n",
    "[data]: (https://datadryad.org/resource/doi:10.5061/dryad.g6g3h). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "Let's get started and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import load_finches_2012\n",
    "df = load_finches_2012()\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** I have added one row of data, simulating the discovery of an \"unknown\" species of finch for which beak measurements have been taken.\n",
    "\n",
    "For pedagogical brevity, we will analyze only beak depth during the class. However, I would encourage you to perform a similar analysis for beak length as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fortis_df = df.query(\"species == 'fortis'\")\n",
    "scandens_df = df.query(\"species == 'scandens'\")\n",
    "unknown_df = df.query(\"species == 'unknown'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fortis_df.shape, scandens_df.shape, unknown_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building\n",
    "\n",
    "We are going to build an estimation model for finch beaks.\n",
    "\n",
    "**Discussion:** Let's think a bit about the data generation process for finch beaks.\n",
    "\n",
    "- What is a reasonable likelihood for the data?\n",
    "- Given that likelihood, what are reasonable distribution (just their names, not their parameter values yet) that we can choose to describe those parameters? (Hint: Think about what are biologically *valid* ranges for finch beaks.)\n",
    "- Given those distributions, what would be reasonable parameter values to describe these hyperpriors?\n",
    "\n",
    "**Exercise:** Recreate the estimation model for finch beak depths. A few things to note:\n",
    "\n",
    "- Practice using numpy-like fancy indexing.\n",
    "- Difference of means & effect size are optional.\n",
    "- Feel free to disagree with the crowd.\n",
    "\n",
    "If it helps, try drawing out the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as beak_depth_model:\n",
    "    # Beaks cannot be of \"negative\" mean, therefore, HalfNormal is \n",
    "    # a reasonable, constrained prior.\n",
    "    mean = pm.HalfNormal('mean', sd=10, shape=(3,))\n",
    "    # Define the prior for the observed variance of the beak depths\n",
    "    sd = pm.HalfCauchy('sd', beta=50, shape=(3,))\n",
    "    # Define the nuisance parameter nu for the T distribution\n",
    "    nu = pm.Exponential('nu', lam=1/29., shape=(3,)) + 1\n",
    "    \n",
    "    # Define the likelihood.\n",
    "    like = pm.StudentT('likelihood', \n",
    "                       nu=nu[df['species_enc']],\n",
    "                       mu=mean[df['species_enc']], \n",
    "                       sd=sd[df['species_enc']], \n",
    "                       observed=df['beak_depth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample from Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with beak_depth_model:\n",
    "    trace = pm.sample(2000, tune=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Posterior Densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the sample trace.\n",
    "traces = az.plot_trace(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the posterior distribution over the population \"mean\" parameter using the forest plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(trace, var_names=['mean']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the posterior distribution of the means using `plot_posterior`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(trace, var_names=['mean']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss:**\n",
    "- Is the posterior distribution of beaks for the unknown species reasonable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code-along: Posterior Predictive Check\n",
    "\n",
    "We will now introduce you to the posterior predictive check. The premise behind this check is that if your model describes the data generating process well, then sampling new data from the data likelihood should give you simulated data that looks like the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pm.sample_ppc(trace, model=beak_depth_model, samples=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Each column in the samples (key: \"likelihood\") corresponds to simulated measurements of each finch in the dataset. We can use fancy indexing along the columns (axis 1) to select out simulated measurements for each category, and then flatten the resultant array to get the full estimated distribution of values for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples_ecdf(ax, group_data, samples):\n",
    "    # Extract just the fortis samples.\n",
    "    group_samples = samples['likelihood'][:, group_data.index].flatten()\n",
    "    # Compute the ECDF for the fortis samples.\n",
    "    x_s, y_s = ECDF(group_samples)\n",
    "    ax.plot(x_s, y_s, label='samples')\n",
    "    \n",
    "def plot_data_ecdf(ax, group_data):\n",
    "    # Compute the ECDF for the fortis samples\n",
    "    x, y = ECDF(group_data['beak_depth'])\n",
    "    ax.plot(x, y, label='data')\n",
    "\n",
    "    \n",
    "def ax_polish(ax, title):\n",
    "    ax.legend()\n",
    "    ax.set_title(title)\n",
    "    \n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(6, 6))\n",
    "\n",
    "ax_fortis, ax_scandens = axes[0], axes[1]\n",
    "\n",
    "plot_samples_ecdf(ax_fortis, fortis_df, samples)\n",
    "plot_samples_ecdf(ax_scandens, scandens_df, samples)\n",
    "\n",
    "plot_data_ecdf(ax_fortis, fortis_df)\n",
    "plot_data_ecdf(ax_scandens, scandens_df)\n",
    "\n",
    "ax_polish(ax_fortis, 'fortis')\n",
    "ax_polish(ax_scandens, 'scandens')\n",
    "ax_scandens.set_xlabel('beak length')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Critique\n",
    "\n",
    "Compare the samples (blue line) to the data (orange line).\n",
    "\n",
    "1. What's similar?\n",
    "1. What's different?\n",
    "1. What modelling choices might be contributing to the differences? How might we modify the model to minimize the difference?\n",
    "\n",
    "Also, think about the reasonable-ness of the estimates for the \"unknown\" species. Is it reasonable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finches Model Rebuild\n",
    "\n",
    "We're going to do a second round of model building, this time incorporating two key ideas:\n",
    "\n",
    "1. We want a hierarchical model, because estimates for the \"unknown\" species fall into the *unreasonable* range. We may want to make this claim because we are relying on other information that we did not bake into the model.\n",
    "1. The Student T distribution may be okay, but a Normal distribution likelihood might be better. Alternatively, we may want to put a prior on the degree of freedom parameter that biases it towards higher values, hence achieving a similar effect as imposing a Normal distribution likelihood. (We would encourage you doing the former, as the latter involves more modelling choices.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-On: Model Building\n",
    "\n",
    "Attempt to build the hierarchical model according to the aforementioned pointers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as hierarchical_finches:\n",
    "    \n",
    "    prior_sig = pm.Exponential(\"prior_sig\", lam=5.)\n",
    "    mu = pm.HalfNormal(\"mu\", sd=prior_sig, shape=(3,))\n",
    "    \n",
    "    prior_lam = pm.Exponential(\"prior_lam\", lam=1.)\n",
    "    sigma = pm.Exponential(\"sigma\", lam=prior_lam, shape=(3,))\n",
    "    \n",
    "    like = pm.Normal(\n",
    "        \"likelihood\", \n",
    "        mu=mu[df['species_enc']],\n",
    "        sd=sigma[df['species_enc']],\n",
    "        observed=df['beak_depth'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample from Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hierarchical_finches:\n",
    "    trace_hierarchical = pm.sample(2000, tune=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(trace_hierarchical, var_names=['mu']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior Predictive Checks\n",
    "\n",
    "Visualize how close our model is to the generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_hierarchical = pm.sample_ppc(trace_hierarchical, model=hierarchical_finches, samples=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(6, 6))\n",
    "\n",
    "ax_fortis, ax_scandens = axes[0], axes[1]\n",
    "\n",
    "plot_samples_ecdf(ax_fortis, fortis_df, samples_hierarchical)\n",
    "plot_samples_ecdf(ax_scandens, scandens_df, samples_hierarchical)\n",
    "\n",
    "plot_data_ecdf(ax_fortis, fortis_df)\n",
    "plot_data_ecdf(ax_scandens, scandens_df)\n",
    "\n",
    "ax_polish(ax_fortis, 'fortis')\n",
    "ax_polish(ax_scandens, 'scandens')\n",
    "ax_scandens.set_xlabel('beak length')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Critique\n",
    "\n",
    "**Discussion:**\n",
    "\n",
    "1. How reasonable are the new estimates for beak length for the unknown species, compared to the old estimates from the non-hierarchical model? Where is its distribution center of mass?\n",
    "1. What about the variance estimates?\n",
    "1. What does the posterior predictive check tell you about the goodness of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1. NumPy-like fancy indexing lets us write models in a concise fashion.\n",
    "1. Posterior estimates can show up as being \"unreasonable\", \"absurd\", or at the minimum, counter-intuitive, if we do not impose the right set of assumptions on the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian-modelling-tutorial",
   "language": "python",
   "name": "bayesian-modelling-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
