{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chapter, you saw how to build what we might call \"multiple\" estimation models.\n",
    "In the example we've been working through, we have gone from\n",
    "estimationg $p$ for a single store to estimating $p$ for 1400+ stores.\n",
    "\n",
    "Something that you might have noticed is that\n",
    "some of the stores had really wide posterior distribution estimates.\n",
    "Depending on your beliefs about the world,\n",
    "this might be considered quite dissatisfying.\n",
    "We might ask, for example, are there really no pieces of information in the data\n",
    "that we might leverage to make more informed inferences\n",
    "about the true like-ability of an ice cream shop?\n",
    "\n",
    "Well, if you remember in the dataset,\n",
    "there was another column that we did not use, `owner_idx`.\n",
    "Let's see if that column might be of any use for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.data import load_ice_cream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_ice_cream()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import janitor\n",
    "import numpy as np\n",
    "\n",
    "naive_p = (\n",
    "    data\n",
    "    .join_apply(  # calculate naive_p\n",
    "        lambda x: \n",
    "            x[\"num_favs\"] / x[\"num_customers\"] \n",
    "            if x[\"num_favs\"] > 0 \n",
    "            else np.nan, \n",
    "        new_column_name=\"naive_p\"\n",
    "    )\n",
    ")\n",
    "\n",
    "(\n",
    "    naive_p\n",
    "    .groupby(\"owner_idx\")\n",
    "    .agg({\"naive_p\": [\"mean\", \"count\", \"std\"]})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.swarmplot(data=naive_p, y=\"naive_p\", x=\"owner_idx\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the visualization, it seems to me that that each of the owners might have a \"characteristic\" $p$,\n",
    "and that each of the owners might also have its own characteristic degree of variability amongst stores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generating Process\n",
    "\n",
    "If we were to re-think our data generating process, we might suggest a slightly modified story.\n",
    "\n",
    "Previously, we thought of our data generating process as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions.estimation import ice_cream_n_group_pgm, ice_cream_one_group_pgm\n",
    "\n",
    "ice_cream_n_group_pgm()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, each shop has its own $p$, and that generates its own \"likes\".\n",
    "Each $p_i$ is drawn from its own Beta distribution,\n",
    "configured with a common $\\alpha$ and $\\beta$.\n",
    "\n",
    "What if we tried to capture the idea that each shop draws its $p$ from its owners?\n",
    "Here's where the notion of hierarchical models comes in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Models\n",
    "\n",
    "In a \"hierarchical\" version of the ice cream shop model,\n",
    "we try to express the idea that not only does each shop have its own $p$,\n",
    "it's $p$ is somehow conditionally dependent on its owner's $p$.\n",
    "\n",
    "More generally, with a hierarchical model,\n",
    "we impose the assumption\n",
    "that each sample draws its key parameters from a \"population\" distribution.\n",
    "Underlying this assumption is the idea\n",
    "that \"things from the same group should be put together\".\n",
    "\n",
    "If we ignored (for a moment) the \"fixed\" variables,\n",
    "then the hierarchical model would look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions.hierarchical import hierarchical_p\n",
    "\n",
    "hierarchical_p()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are expressing the idea that each shop $i$ draws its $p_{j, i}$ from its the $p_{j}$ associated with its owner $j$,\n",
    "and that its owner $p_{j}$ draws from a population $p$ distribution governing all owners.\n",
    "\n",
    "In theory, this is really cool.\n",
    "But implementing this is kind of difficult,\n",
    "if we think more closely about the structure we've used thus far.\n",
    "With Beta distributions as priors,\n",
    "we might end up with a very convoluted structure instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions.hierarchical import convoluted_hierarchical_p\n",
    "\n",
    "convoluted_hierarchical_p()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure how you feel looking at that PGM diagram,\n",
    "but at least to me, it looks convoluted.\n",
    "I'd find it a hassle to implement.\n",
    "Also, I wouldn't be able to bake in interpretability into the model\n",
    "by directly mapping key parameter values to quantities of interest.\n",
    "\n",
    "The key problem here is that of _parameterization_.\n",
    "By _directly_ modelling $p$ with a Beta distribution,\n",
    "we are forced to place priors on the $\\alpha$ and $\\beta$ parameters\n",
    "of the Beta distribution.\n",
    "That immediately precludes us\n",
    "from being able to model the \"central tendencies\"\n",
    "of owner-level shop ratings.\n",
    "\n",
    "To get around this, I'm going to introduce you to this idea\n",
    "of transforming a random variable,\n",
    "which is immensely helpful in modelling tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations of random variables\n",
    "\n",
    "In our application,\n",
    "being able to model directly the \"central tendency\" of the $p$,\n",
    "for each shop and owner, matters a lot.\n",
    "\n",
    "A Beta distribution parameterization does not allow us\n",
    "to model $p$ with \"central tendencies\" directly.\n",
    "\n",
    "On the other hand, if we were to \"transform\" the random variable $p$,\n",
    "which has bounded support between 0 and 1,\n",
    "into a regime that did not have a bounded support,\n",
    "we could conveniently use Gaussian distributions,\n",
    "which have central tendency parameters that we can model\n",
    "using random variables directly.\n",
    "\n",
    "### Logit Transform\n",
    "\n",
    "One such transformation for a random variable that is bounded\n",
    "is the **logit transform**.\n",
    "In math form, given a random variable $p$ that is bounded in the $[0, 1]$ interval,\n",
    "the logit transformation like this:\n",
    "\n",
    "$$f(p) = \\log(\\frac{p}{1-p})$$\n",
    "\n",
    "To help you understand a bit of the behaviour of the logit function, here it is plotted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import logit\n",
    "import seaborn as sns\n",
    "\n",
    "p = np.linspace(0, 1, 1000)\n",
    "logit_p = logit(p)\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "plt.plot(p, logit_p)\n",
    "plt.xlabel(\"p\")\n",
    "plt.ylabel(\"logit(p)\")\n",
    "sns.despine();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of the Logit Transformation\n",
    "\n",
    "As you can see, the logit transformation function maps values of $p$,\n",
    "which live on the interval between 0 and 1,\n",
    "onto an interval that is in the interval $(-\\infty, \\infty)$.\n",
    "\n",
    "It starts with the **odds ratio** term, which is $\\frac{p}{1-p}$,\n",
    "which is a ratio of the probability of getting an outcome\n",
    "to the probability of not getting the outcome.\n",
    "We then take the odds ratio, and log-transform it.\n",
    "When the probability of obtaining an outcome is less than 0.5,\n",
    "we end up in the negative regime,\n",
    "and when the probability of obtaining an outcome is greater than 0.5,\n",
    "we end up in the positive regime.\n",
    "\n",
    "Remember also that we desired a way to model the central tendencies of our random variables,\n",
    "and so a highly natural choice here is to use the Gaussian distribution,\n",
    "which has a central tendency parameter $\\mu$.\n",
    "And since the logit of $p$ has infinite support,\n",
    "we can use a distribution that has infinite support to model it.\n",
    "As such, we can instantiate a random variable for the _logit transformed version of $p$_,\n",
    "and then use the inverse logit transformation （also known as the `expit` function in `scipy.special`)\n",
    "to take it back to bounded $(0, 1)$ space,\n",
    "which we can then use for our Binomial likelihood function for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Explore the transformation\n",
    "\n",
    "Use the widgets below to explore how the transformation between the logit ($f(p)$) and original $p$ maps onto one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions.hierarchical import plot_mu_p\n",
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "mu = FloatSlider(value=0.5, min=-3, max=3, step=0.1)\n",
    "sigma = FloatSlider(value=1, min=0.1, max=5, step=0.1)\n",
    "\n",
    "interact(plot_mu_p, mu=mu, sigma=sigma);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice a few things:\n",
    "\n",
    "1. $\\mu$ controls the central tendency of the bounded space $p$.\n",
    "2. $\\sigma$ controls the variance of the bounded space $p$.\n",
    "\n",
    "With this transformation trick, it's possible to model both the central tendencies and the variance _directly_!\n",
    "\n",
    "Let's see how this can get used, by redoing our model's PGM with the alternative parametrization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions.hierarchical import hierarchical_pgm\n",
    "\n",
    "hierarchical_pgm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we read this new PGM?\n",
    "\n",
    "This is how we read it.\n",
    "\n",
    "- The \"red\" random variables are transformed versions of $p$ from their respective $\\mu$s.\n",
    "- The $\\mu$s are hierarchically related, which gives us the central tendencies.\n",
    "- The uncertainty in $\\mu$ values (at all levels) are modeled by a variance term.\n",
    "- Some of the variance terms are fixed, while others are modelled by a random variable; this is a modelling choice.\n",
    "    - In setting up this problem I had this idea that analyzing the variance term of each owner might be handy, so I've included it in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical model\n",
    "\n",
    "Here is the hierarchical model written down in PyMC3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions.hierarchical import ice_cream_hierarchical_model\n",
    "ice_cream_hierarchical_model??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ice_cream_hierarchical_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"owner_idx\"].sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import pymc3 as pm\n",
    "\n",
    "with model:\n",
    "    trace = pm.sample(2000, tune=2000)\n",
    "    trace = az.from_pymc3(\n",
    "        trace,\n",
    "        coords={\n",
    "            \"p_shop_dim_0\": data[\"shopname\"],\n",
    "            \"logit_p_shop_dim_0\": data[\"shopname\"],\n",
    "            \"logit_p_owner_scale_dim_0\": data[\"owner_idx\"].sort_values().unique(),\n",
    "            \"p_owner_dim_0\": data[\"owner_idx\"].sort_values().unique(),\n",
    "            \"logit_p_owner_mean\": data[\"owner_idx\"].sort_values().unique(),\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to ask you to ignore the warnings about divergences for a moment, we will get there in the next chapter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, var_names=[\"p_owner\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(trace, var_names=[\"p_owner\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation In Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Owner-level $p$\n",
    "\n",
    "Analysis of the posterior $p$s for the **owners** tells us that different owners have different characteristic $p$.\n",
    "\n",
    "We can see this from the forest plot below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(trace, var_names=[\"p_owner\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, it seems clear to me that the shops belonging to owners 2, 3 and 5 are generally unfavourable,\n",
    "while the shops belonging to owners 6, 7 and 8 are the best.\n",
    "\n",
    "Of worthy mention is owner 8, which is actually the set of independently-owned shops.\n",
    "Those shops are, in general, very well-rated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Variation\n",
    "\n",
    "I mentioned earlier that I thought that the variance of the owners' characteristic logits might be interesting to analyze,\n",
    "and the reason is as such:\n",
    "If an owner's estimated $\\sigma$ is large, that means that the shops might be quite _inconsistent_ in how much customers like them.\n",
    "If customer service is the primary driver of how good their customers like them,\n",
    "then that could be actionable information for owners to tighten up on customer service training.\n",
    "\n",
    "At the same time, a tight distribution (small $\\sigma$) coupled with poor ratings means something systematically bad might be happening.\n",
    "\n",
    "Well, enough with the hypothesizing, let's dive in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(trace, var_names=[\"logit_p_owner_scale\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to plot the _joint_ posterior distributions for each of the owners' $p$ and $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = trace.posterior[\"p_owner\"].to_dataframe().unstack(-1)\n",
    "scales = trace.posterior[\"logit_p_owner_scale\"].to_dataframe().unstack(-1)\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    plt.scatter(locations[(\"p_owner\", i)], scales[(\"logit_p_owner_scale\", i)], alpha=0.3, label=f\"{i}\")\n",
    "plt.xlabel(\"owner p\")\n",
    "plt.ylabel(\"owner $\\sigma$\")\n",
    "sns.despine()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the full posterior distribution of owner $\\sigma$ against owner $p$,\n",
    "we can immediately see how some owners are really good (to the right on the $p$ axis)\n",
    "and very consistent (closer to the bottom on the $\\sigma$ axis).\n",
    "\n",
    "You might also notice that some of the shapes above look \"funnel-like\".\n",
    "I have intentionally placed this plot here\n",
    "as a foreshadowing of what we'll be investigating in the next chapter,\n",
    "and it's related to the divergences that we saw above.\n",
    "Those are what we will be diving deeper into later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation In Context\n",
    "\n",
    "By plotting the $p$ of the owner against the posterior distribution variance,\n",
    "we can visualize the two pointers made above in a way that communicates really clearly\n",
    "which owners might need help.\n",
    "\n",
    "Qualitatively-speaking, owners would ideally want to be in the bottom right quadrant of the plot.\n",
    "That is where ratings are high and there's very little variability.\n",
    "Owner 7 fits that bill very nicely, as does owner 6.\n",
    "The independent shops are overall very highly rated, but they aren't very consistent;\n",
    "this is the top-right quadrant of the plot.\n",
    "\n",
    "The worst place to be in is the bottom-left: poor customer ratings, and consistently so.\n",
    "We might devise further hypotheses as to why:\n",
    "bad hygiene standards,\n",
    "lack of training across the board,\n",
    "some other historical factor etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Individual Shops\n",
    "\n",
    "One of the promises of using a Bayesian hierarchical model here\n",
    "is the ability to draw _tentative_ conclusions,\n",
    "conditioned on our model's assumptions,\n",
    "about the state of certain shops\n",
    "_even in the low or zero data regime_.\n",
    "In the machine learning world,\n",
    "one might claim that this is a form of transfer learning,\n",
    "or that it is form of one-shot learning.\n",
    "I'd prefer not to be quoted on that,\n",
    "so I'll just call it what it actually is:\n",
    "inference about the state of the world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A comparison of naive, bayesian estimated, and shop-level $p$s\n",
    "\n",
    "One thing we are going to do here is extract out the naive estimates,\n",
    "which will contain nulls because of a lack of data,\n",
    "the Bayesian estimated $p$s, which will be fully populated,\n",
    "and compare them both against the shop-level $p$s.\n",
    "We should see the effects of a hierarchical model here:\n",
    "for each store, the $p$ will be centered on the owner's $p$,\n",
    "but there will be variation around it.\n",
    "\n",
    "The next few code cells will explicitly show how we gather out the necessary summary statistics,\n",
    "while also highlighting the use of `pyjanitor`,\n",
    "a library that I have developed to munge data with a clean, Pythonic API.\n",
    "\n",
    "Firstly, we grab out the Bayesian estimates from the posterior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import janitor\n",
    "\n",
    "bayesian_estimates = (\n",
    "    trace.posterior\n",
    "    .stack(draws=(\"chain\", \"draw\"))\n",
    "    .median(dim=\"draws\")\n",
    "    [\"p_shop\"]\n",
    "    .to_dataframe()\n",
    "    .reset_index()\n",
    "    .rename_column(\"p_shop_dim_0\", \"shopname\")\n",
    "    .rename_column(\"p_shop\", \"bayesian_p\")\n",
    "    .set_index(\"shopname\")\n",
    ")\n",
    "bayesian_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we grab out the owner-level $p$.\n",
    "In principle I would have used the posterior distribution,\n",
    "but a naive estimate quickly calculated from the naive data\n",
    "will be very close in our case.\n",
    "(If you have the notebooks open in Binder,\n",
    "you should definitely give it a shot\n",
    "extracting the estimates from the posterior samples instead!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owner_p = (\n",
    "    naive_p\n",
    "    .groupby_agg(\"owner_idx\", new_column_name=\"owner_p\", agg_column_name=\"naive_p\", agg=\"mean\")\n",
    "    .set_index(\"shopname\")\n",
    "    .select_columns([\"owner_p\"])\n",
    ")\n",
    "owner_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's join everything together into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shrinkage = (\n",
    "    naive_p\n",
    "    .set_index(\"shopname\")\n",
    "    .select_columns([\"naive_p\", \"owner_idx\"])\n",
    "    .join(bayesian_estimates)\n",
    "    .join(owner_p)\n",
    ")\n",
    "shrinkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already, one of the advantages of a Bayesian estimate shows up:\n",
    "we are able to fill in the NaN values left behind by a naive estimate\n",
    "when no data are available.\n",
    "How was this possible?\n",
    "It was possible because the _structure_ of our model\n",
    "presumed that each store drew its $\\mu$ (and hence $p$) from the owner's $\\mu$ (and hence $p$),\n",
    "thus we obtain an estimate for the store,\n",
    "which will look similar to the owner's $p$.\n",
    "\n",
    "Let's visualize a comparison of the Bayesian $p$ estimates\n",
    "against the naive and store-level $p$ estimates. \n",
    "We are going to construct a \"shrinkage\" plot.\n",
    "(This is a diagnostic plot you can use\n",
    "to help others visualize the comparison\n",
    "we are about to go through.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Dropdown\n",
    "\n",
    "owner_idx = Dropdown(options=list(range(9)), description=\"Owner\")\n",
    "owner_idx\n",
    "\n",
    "@interact(owner_idx=owner_idx)\n",
    "def plot_shrinkage(owner_idx):\n",
    "    data = (\n",
    "        shrinkage\n",
    "        .query(\"owner_idx == @owner_idx\")\n",
    "        .select_columns([\"naive_p\", \"bayesian_p\", \"owner_p\"])\n",
    "    )\n",
    "    nulls = (\n",
    "        data.dropnotnull(\"naive_p\")\n",
    "    )\n",
    "    non_nulls = (\n",
    "        data.dropna(subset=[\"naive_p\"])\n",
    "    )\n",
    "    fig, axes = plt.subplots(figsize=(8, 4), nrows=1, ncols=2 , sharey=True, sharex=True)\n",
    "    non_nulls.T.plot(legend=False, color=\"blue\", alpha=0.1, marker='o', ax=axes[1], title=\"has data\",)\n",
    "    nulls.T.plot(legend=False, color=\"blue\", alpha=0.1, marker='o', ax=axes[0], title=\"no data\")\n",
    "    axes[0].set_ylabel(\"Estimated $p$\")\n",
    "    \n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left plot shows the estimates for shops that have zero data.\n",
    "Rather than estimate that its performance is unknowable,\n",
    "we estimate that each shop's performance will be pretty close to\n",
    "the owner-level $p$.\n",
    "\n",
    "The right plot shows the estimates for shops that _do_ have data.\n",
    "Those shops that have 1 out of 1 or 0 out of 1 no longer are estimated to have\n",
    "a rating of 100% or 0% (respectively),\n",
    "but rather are estimated to have their ratings closer to the owner's $p$.\n",
    "\n",
    "As you should be able to see, the Bayesian estimates for store's $p$\n",
    "are _shrunk_ towards the owner-level $p$ estimates\n",
    "relative to the naive $p$ estimates.\n",
    "This phenomena is called \"shrinkage\".\n",
    "\n",
    "Shrinkage in and of itself is a neutral thing.\n",
    "Whether it is \"good\" or \"bad\" depends on the problem being solved.\n",
    "In this case, I might consider shrinkage to be good,\n",
    "because it is preventing us from giving wildly bad guesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where might the hierarchical modelling assumption be a bad thing?\n",
    "\n",
    "In this chapter, we have gone in-depth about how hierarchical modelling can be a useful tool\n",
    "to mathematically bake in the assumption that \"birds of a feather flock together\".\n",
    "When this modelling assumption has no _serious_ detrimental effects,\n",
    "it could be handy.\n",
    "\n",
    "On the other hand, in an article titled [Meet the Secret Algorithm That's Keeping Students Out of College][wired] on Wired,\n",
    "a highly revealing paragraph illuminated for me one scenario where this assumption could instead be potentially highly detrimental.\n",
    "\n",
    "The backdrop here is that in 2020, because of the COVID-19 outbreak, International Baccalaureate examinations worldwide were cancelled,\n",
    "and so the IB board had to come up with a method to grade students.\n",
    "Other standardized testing exams, such as the Cambridge University's GCEs and the SATs,\n",
    "just went ahead with online tests,\n",
    "but the IB board went with a model instead:\n",
    "\n",
    "> The idea was to use prior patterns to infer what a student would have scored in a 2020 not dominated by a deadly pandemic. IB did not disclose details of the methodology but said grades would be calculated based on a student’s assignment scores, predicted grades, and historical IB results from their school. The foundation said grade boundaries were set to reflect the challenges of remote learning during a pandemic. For schools where historical data was lacking, predictions would build on data pooled from other schools instead.\n",
    "\n",
    "Grading individual students using information from their school;\n",
    "borrowing information from other schools where not enough historical information for a school was present...\n",
    "These all sound oddly familiar to the kind of thing we've done with ice cream shops.\n",
    "The only thing here is that the consequences of using a model could be heavily life-shaping for individual students.\n",
    "Also, the amount of agency afforded to the individual students to influence their grades on a final exam is removed.\n",
    "I'm going to withold judgment on whether that is good or bad,\n",
    "though I will state my personal preference for consistently good performance over a long run\n",
    "rather than one-time tests that may be subject to a lot of noise.\n",
    "\n",
    "Here, the use of a model may fundamentally be an unfair idea,\n",
    "if we cannot disentangle long-run performance from confounders in the data.\n",
    "What are your thoughts after reading the article?\n",
    "\n",
    "[wired]: https://www.wired.com/story/algorithm-set-students-grades-altered-futures/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving posterior traces\n",
    "\n",
    "Knowing how to save posterior distribution traces is really handy,\n",
    "as it allows us the chance to examine and compare model posterior distributions\n",
    "given different model structures.\n",
    "(That is what we'll be going through in the next notebook.)\n",
    "\n",
    "Let's see how to use ArviZ to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyprojroot import here\n",
    "\n",
    "save_path = here() / \"data/ice_cream_shop_hierarchical_posterior.nc\"\n",
    "az.to_netcdf(trace, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian-modelling-tutorial",
   "language": "python",
   "name": "bayesian-modelling-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
